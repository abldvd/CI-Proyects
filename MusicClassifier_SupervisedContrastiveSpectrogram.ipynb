{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicClassifier - SupervisedContrastiveSpectrogram.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORoa7sDrkwM98Rm++Yr2Jz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abldvd/CI-Proyects/blob/main/MusicClassifier_SupervisedContrastiveSpectrogram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised contrastive code source: https://keras.io/examples/vision/supervised-contrastive-learning/#supervised-contrastive-learning"
      ],
      "metadata": {
        "id": "14K3G7fDFtfp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT2ajL7xd7SK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01203672-17fa-47e9-fa45-a6a7bd106354"
      },
      "source": [
        "!pip install pydub\n",
        "!pip install tensorflow-addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.15.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEpM_q484tny",
        "outputId": "c9a046dd-e7ee-4b6b-9b26-06ff0dc43bf0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjwjdQN8ZX_L",
        "outputId": "e7fd32da-dabc-400b-d9ec-0dec2f8a0bd2"
      },
      "source": [
        "PATH = 'drive/MyDrive/Colab Notebooks/datasets/music_files/test'\n",
        "MAX_AUDIO_LEN = 60\n",
        "BIN_SIZE = 2**11\n",
        "!ls drive/MyDrive/'Colab Notebooks'/datasets/music_files/test"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classical  Rock  Synthwave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt5aeFV_ZefS"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import librosa\n",
        "import numpy as np\n",
        "import random as rd\n",
        "from pydub import AudioSegment \n",
        "from pydub.utils import make_chunks\n",
        "from scipy.io import wavfile\n",
        "from tempfile import mktemp\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcdnkMggcN5S"
      },
      "source": [
        "def mp32wav(mp3_path, len=None):\n",
        "  mp3_audio = AudioSegment.from_file(mp3_path, format=\"mp3\")  # read mp3\n",
        "  if len and len > mp3_audio.duration_seconds:\n",
        "      raise Exception('Fixed lenght greater than file lenght')\n",
        "  wname = mktemp('.wav')  # use temporary file\n",
        "  if len:\n",
        "    random_midpoint = np.random.randint(int(len*1000/2), int(mp3_audio.duration_seconds*1000 - len*1000/2))\n",
        "    mp3_audio = mp3_audio[\n",
        "      random_midpoint - len*1000/2:  \n",
        "      random_midpoint + len*1000/2] # crop and save to wav\n",
        "  mp3_audio.export(wname, format=\"wav\")  \n",
        "  rate, audio = wavfile.read(wname)  # read as wav file\n",
        "  os.remove(wname) # dont want leaks here\n",
        "  return audio, rate"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu-Nx0GjcC0X"
      },
      "source": [
        "def getSpectrogram(mp3_path, bin_size, len): \n",
        "  audio, rate = mp32wav(mp3_path, len)  # get wave file  \n",
        "  audio = np.mean(audio, axis=1)\n",
        "\n",
        "  spectrum = librosa.stft(audio, n_fft=bin_size, hop_length=int(rate))\n",
        "  return spectrum[:, :-1].reshape(spectrum[:, :-1].shape+(1,))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbjSNeVXZ_ny"
      },
      "source": [
        "def loadSpectrumData(path, seed=1234, file_limit=200):\n",
        "  # Loads data from a main folder, having the option to split into validation or training \n",
        "  first_file_shape = getSpectrogram(f'{path}/{os.listdir(path)[0]}/{os.listdir(f\"{path}/{os.listdir(path)[0]}\")[0]}', BIN_SIZE, MAX_AUDIO_LEN).shape\n",
        "  num_files = int(sum([len(os.listdir(f'{path}/{class_folder}')[:file_limit]) for class_folder in os.listdir(path)]))\n",
        "  num_classes = len(os.listdir(path))\n",
        "                             # Inits   \n",
        "  X = np.zeros((num_files,) + first_file_shape, dtype = 'complex_')  # Lets assume every spectrum will have the same shape\n",
        "  y = np.zeros(num_files, dtype=str)\n",
        "\n",
        "  rd.seed(seed)\n",
        "  last_i = 0\n",
        "  for class_folder in os.listdir(path): # Iterating over the classes\n",
        "    file_list = os.listdir(f'{path}/{class_folder}')\n",
        "    rd.shuffle(file_list)\n",
        "    if file_limit:       # Appliying memory limits and randomizing\n",
        "      file_list = file_list[:file_limit]\n",
        "      rd.shuffle(file_list)\n",
        "\n",
        "    for i, file_name in enumerate(file_list): \n",
        "      try:                 \n",
        "                              # Iterating and loading spectrum\n",
        "        X[last_i+i,] = getSpectrogram(f'{path}/{class_folder}/{file_name}', BIN_SIZE, MAX_AUDIO_LEN)\n",
        "\n",
        "      except ValueError as v: # Our assumption was wrong, some files had a rounding error and had one less half a second\n",
        "        missed_file = getSpectrogram(f'{path}/{class_folder}/{file_name}', BIN_SIZE, MAX_AUDIO_LEN) \n",
        "        X[last_i+i,] = np.c_[missed_file, np.ones(np.shape(missed_file)[0], dtype = 'complex_')] # Lets add something so we dont lose the sample\n",
        "      y[last_i+i] = class_folder\n",
        "    last_i += i+1\n",
        "        \n",
        "  lb = LabelEncoder() # Transforming y to categorical\n",
        "  y = keras.utils.to_categorical(lb.fit_transform(y), num_classes=num_classes)\n",
        "  return X,  y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkRfAuHHUi_-"
      },
      "source": [
        "# DATA -------------------------------------------------------------------------\n",
        "num_classes = len(os.listdir(PATH))\n",
        "input_shape = getSpectrogram(f'{PATH}/{os.listdir(PATH)[0]}/{os.listdir(f\"{PATH}/{os.listdir(PATH)[0]}\")[0]}', BIN_SIZE, MAX_AUDIO_LEN).shape\n",
        "X, y = loadSpectrumData(PATH)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXLVPnQDTYo-"
      },
      "source": [
        "# AUGMENTATION -----------------------------------------------------------------\n",
        "class ChunkRandomizer(layers.Layer):\n",
        "  def __init__(self, cuts=6, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.cuts = cuts\n",
        "\n",
        "  def call(self, spect):\n",
        "    spect = tf.split(spect, self.cuts, axis=2)\n",
        "    rd.shuffle(spect)\n",
        "    return tf.concat(spect, axis=2)\n",
        "\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        ChunkRandomizer(),\n",
        "        layers.GaussianNoise(0.3)\n",
        "    ]\n",
        ")\n",
        "data_augmentation.layers[0].adapt(X_train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdBO5g-JzYZG"
      },
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 44\n",
        "hidden_units = 128\n",
        "projection_units = 256\n",
        "num_epochs = 100\n",
        "temperature = 0.1\n",
        "\n",
        "# MODEL ------------------------------------------------------------------------\n",
        "\n",
        "def create_encoder():\n",
        "    encoder = keras.Sequential()\n",
        "     \n",
        "    encoder.add(layers.Input(input_shape))\n",
        "    encoder.add(data_augmentation)\n",
        "    encoder.add(layers.Conv2D(32, kernel_size=(4, 2), activation='relu'))\n",
        "    encoder.add(layers.Dropout(0.15))\n",
        "    encoder.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    encoder.add(layers.Conv2D(64, kernel_size=(4, 2), activation='relu'))\n",
        "    encoder.add(layers.Dropout(0.25))\n",
        "    encoder.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    encoder.add(layers.Conv2D(128, kernel_size=(4, 2), activation='relu'))\n",
        "    encoder.add(layers.Dropout(0.5))\n",
        "    encoder.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    encoder.add(layers.Flatten())\n",
        "    encoder.add(layers.Dense(hidden_units))\n",
        "\n",
        "    return encoder\n",
        "\n",
        "\n",
        "def create_classifier(encoder, trainable=True):\n",
        "    for layer in encoder.layers[:-1]:\n",
        "        layer.trainable = trainable\n",
        "\n",
        "    classifier = keras.Sequential()\n",
        "    classifier.add(layers.Input(input_shape)) \n",
        "    classifier.add(encoder)\n",
        "    classifier.add(layers.Dense(hidden_units, activation=\"relu\"))\n",
        "    classifier.add(layers.Dropout(0.20))\n",
        "    classifier.add(layers.Dense(hidden_units/4, activation=\"relu\"))\n",
        "    classifier.add(layers.Dropout(0.10))\n",
        "    classifier.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "  \n",
        "    \n",
        "    return classifier\n",
        "\n",
        "\n",
        "def add_projection_head(encoder):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    features = encoder(inputs)\n",
        "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
        "    model = keras.Model(\n",
        "        inputs=inputs, outputs=outputs, name=\"encoder-with_projection_head\"\n",
        "    )\n",
        "    return model\n",
        "\n",
        "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):  \n",
        "        \n",
        "        \n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_multilabel_loss(y_true=labels, y_pred=logits)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi-FT-srXm2V",
        "outputId": "7a5a25a8-21f2-4a56-f36f-8f2da29c8814"
      },
      "source": [
        "# PRETRAINING ------------------------------------------------------------------\n",
        "encoder = create_encoder()\n",
        "\n",
        "encoder_with_projection_head = add_projection_head(encoder)\n",
        "encoder_with_projection_head.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=SupervisedContrastiveLoss(temperature),\n",
        ")\n",
        "\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
        "\n",
        "\n",
        "history = encoder_with_projection_head.fit(\n",
        "    x=X_train, y=y_train, batch_size=batch_size, epochs=num_epochs, callbacks = [es]\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 56s 5s/step - loss: 3.6209\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 57s 6s/step - loss: 3.4203\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.3578\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 56s 6s/step - loss: 3.2792\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 56s 6s/step - loss: 3.3860\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 56s 6s/step - loss: 3.2875\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 56s 6s/step - loss: 3.2293\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.2485\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.2465\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.2143\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 60s 6s/step - loss: 3.2012\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.1764\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.1557\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.1561\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 56s 6s/step - loss: 3.1860\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.1336\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.1522\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 3.1294\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 56s 6s/step - loss: 3.0896\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 54s 5s/step - loss: 3.0515\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.9813\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 59s 6s/step - loss: 2.9935\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.9683\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.9464\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.9513\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.8901\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.8835\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.8739\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 56s 6s/step - loss: 2.8810\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.8624\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 54s 5s/step - loss: 2.8544\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 55s 6s/step - loss: 2.8166\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 54s 5s/step - loss: 2.8099\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 54s 5s/step - loss: 2.7366\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 54s 5s/step - loss: 2.8312\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 54s 5s/step - loss: 2.7938\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.7494\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 54s 5s/step - loss: 2.7462\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 55s 5s/step - loss: 2.7146\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 54s 5s/step - loss: 2.7069\n",
            "Epoch 41/100\n",
            " 6/10 [=================>............] - ETA: 23s - loss: 2.7769"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U0Ua6ldzgPw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Encoder Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbFollCkzc7b"
      },
      "source": [
        "# TRAINING --------------------------------------------------\n",
        "classifier = create_classifier(encoder, trainable=False)\n",
        "\n",
        "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1, patience=15, restore_best_weights=True)\n",
        "\n",
        "classifier.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.CategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.CategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "history = classifier.fit(x=X_train, y=y_train, batch_size=batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks = [es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XcAEdBywtmw"
      },
      "source": [
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('categorical_accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation','loss'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF6wK0a9T34K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}